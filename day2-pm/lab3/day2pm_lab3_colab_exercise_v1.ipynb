{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/aiup/blob/main/day2-pm/lab3/day2pm_lab3_colab_exercise_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBQPP42Yexg3"
      },
      "source": [
        "<img src=\"https://www.nyp.edu.sg/content/dam/nyp/logo.png\" width='200'/>\n",
        "\n",
        "Welcome to the lab! Before we get started here are a few pointers on Jupyter notebooks.\n",
        "\n",
        "1. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
        "\n",
        "2. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
        "\n",
        "3. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEbvITAFexg4"
      },
      "source": [
        "# Lab 3 - Using NVidia Nemo for Speech Recognition\n",
        "\n",
        "In this exercise, we will learn how to make use of NVidia's NeMo library to help create our Speech Recognition models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIAkx6i0exg5"
      },
      "source": [
        "## Section 3.1 - Download / Import The Necessary Modules\n",
        "\n",
        "Run the following cell to download the datasets, import the necessary modules and set up the folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc5B3Ct_exg5",
        "scrolled": true
      },
      "source": [
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install Cython\n",
        "!pip install nemo_toolkit['ASR']\n",
        "!pip install ipywebrtc==0.5.0\n",
        "!pip install ffmpeg-python\n",
        "!pip install omegaconf\n",
        "!wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/aiup/day2-pm/lab3/lab3.zip\n",
        "!unzip lab3.zip\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'main'\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs\n",
        "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/config.yaml\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsCOxdngVGXy"
      },
      "source": [
        "## IMPORTANT - Restart Runtime\n",
        "\n",
        "You must restart your Colab session to allow the latest libraries that we just downloaded to take effect. \n",
        "\n",
        "To do so, either:\n",
        "\n",
        "1. Click on the \"Restart Runtime\" button in the output cell above, OR\n",
        "2. Click on the menu \"Runtime\" > \"Restart runtime\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc4nZgTRVGXz"
      },
      "source": [
        "from helpers import *\n",
        "print (\"Import helpers complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgOC_qQRexg9"
      },
      "source": [
        "## Section 3.2 - Using a Pre-Trained Nemo Model\n",
        "\n",
        "Nvidia provides many pre-trained models for English and Chinese. You can find the list available here: https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels\n",
        "\n",
        "The model that we will be using is the 'QuartzNet15x5Base-En' model. The QuartzNet model architecture looks like the following:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/day2-pm/quartznet.png\" />\n",
        "\n",
        "https://arxiv.org/pdf/1910.10261.pdf\n",
        "\n",
        "\n",
        "According to Nemo's website above, this model has been trained on a large number of speech datasets, including the Singapore National Speech Corpus (English). In theory, this model should work well in recognizing local accents, and you can go ahead to put it to the test!\n",
        "\n",
        "Update the following cell to pass in the config file's path **\"configs/config.yaml\"** into the first parameter, and the pre-trained model name  **\"QuartzNet15x5Base-En\"** into the second parameters.\n",
        "\n",
        "Run the cell to download the pre-trained model from NVidia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5ztFj6Eexg-"
      },
      "source": [
        "# TODO:\n",
        "# Set the path to the configuration file and the model name\n",
        "# NOTE: The details of the neural network specified in the config file is\n",
        "#       ignored in this case, because we will be downloading the pre-trained neural network.\n",
        "#\n",
        "create_speech_recognition_nemo_model_pretrained(\"configs/config.yaml\", \"???\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZSTMVgpexhA"
      },
      "source": [
        "## Section 3.3 - Using the Pre-Trained Speech Recognition Model for Transcription\n",
        "\n",
        "Take a look at the \"video.mp4\" file in the data folder. It is a video available in the public domain at: https://archive.org/details/GoodEati1951\n",
        "\n",
        "What we will do now is to extract part of the audio clip from the movie file and send it to our pre-trained speech recognition model to transcribe.\n",
        "\n",
        "To do so, update the following code to:\n",
        "\n",
        "1. Set the first parameter for the movie file path to the **\"data/video.mp4\"** file, \n",
        "2. Set the second parameter for the start time (in seconds) of the audio to extract, for example, **0.0**\n",
        "3. Set the third parameter for the end time (in seconds) of the audio to extract, for example, **120.0**.\n",
        "\n",
        "See if the transcription using NVidia's Speech Recognition model look reasonable based on what you heard from the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhdd81gDexhB"
      },
      "source": [
        "# TODO:\n",
        "# Update the parameters to specify the movie file, and the audio start/end to\n",
        "# transcribe.\n",
        "#\n",
        "perform_transcription_on_file(extract_audio_clip(\n",
        "    \"???\",            # Path to the movie file.\n",
        "    0.0,              # Start time of audio (in seconds)\n",
        "    0.0))             # End time of audio (in seconds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVQkD1xrexhD"
      },
      "source": [
        "Now, we are going to try using a recording to test our pre-trained Speech Recognition model. \n",
        "\n",
        "Run the following cell to display an audio recorder panel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FixurGIexhD"
      },
      "source": [
        "display_audio_recorder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnftIqblexhG"
      },
      "source": [
        "Then, update the cell below to pass in the **save_recorded_audio()** function into the perform_transcription_on_file() function.\n",
        "\n",
        "This has the effect of converting the recorded audio into a WAV format and then sending it into the Nemo speech recognition model.\n",
        "\n",
        "Take a look at the output transcription and see if it matches to what you said."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSt75c6-exhH"
      },
      "source": [
        "# TODO:\n",
        "# Pass the function 'save_recorded_audio()' into the parameter but without\n",
        "# the single quotation marks \"'\"\n",
        "#\n",
        "perform_transcription_on_file(\"???\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY4zl6DuexhK"
      },
      "source": [
        "## Section 3.4 - Processing Our Custom AN4 Dataset\n",
        "\n",
        "Let's assume that we want to train our own custom speech recognition model that can help us recognize people spelling out words and saying numbers. There is an existing dataset, called the AN4 dataset, that contains a collection of audio files of people spelling persons' names, cities and states, or speaking out numbers, or simple words. This dataset was developed by Carnegie Mellon University for research purposes, and is available here: http://www.speech.cs.cmu.edu/databases/an4/\n",
        "\n",
        "We have already downloaded the AN4 dataset from the link above and set it up in your Virtual Machine in the **\"data/an4\"** folder. \n",
        "\n",
        "The CMU's AN4 dataset contains audio files recorded in the SPH format. But Nemo only recognizes WAV formats, so we need to first load all the SPH files and individually convert them to the WAV format.\n",
        "\n",
        "Update the following code by passing in the \"folder\" variable that we declared above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpNG20VQexhK",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Update the code below to pass in the folder path containing \n",
        "# all our SPH files.\n",
        "#\n",
        "convert_sph_files_to_wav(\"???\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgmjrp-XexhN"
      },
      "source": [
        "## Section 3.5 - Exploring Our Data\n",
        "\n",
        "Let's take a look at one of the files that we have converted and see how an audio waveform looks like. This is the sound of a person saying the letters \"G L E N N\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZMSFHEMexhO",
        "scrolled": true
      },
      "source": [
        "audio_file = 'data/an4/wav/an4_clstk/mgah/cen2-mgah-b.wav'\n",
        "playback_audio(audio_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyRrsPVoexhQ"
      },
      "source": [
        "The following shows how the waveform actually looks like when plotted in a graph. If the audio recording contains distinct words or letters with pauses in between, you will be able to see large segments of \"pertubations\" where the speaker is speaking, and flat lines when the speaker makes short pauses. The \"pertubations\" represents vibrations as the sound wave travels through air and gets picked up by the microphone or our ear. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arvvD0GzexhQ"
      },
      "source": [
        "display_audio_waveform(audio_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_df8ZXZQexhT"
      },
      "source": [
        "A spectrogram is a signal processing technique that converts a signal like an audio waveform above into its time-frequency components. Let's take a look at how that piece of audio looks like as a spectrogram. \n",
        "\n",
        "It tells you, at each time step, which frequencies (or the pitch / tune) are \"playing\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNuEOEIlexhU"
      },
      "source": [
        "display_audio_spectrogram(audio_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgvjNvwtexhV"
      },
      "source": [
        "Then, see how the same piece of audio looks like when the y-axis (the frequency axis) is mel-scaled. A mel-scale is a scale of pitches where human listeners perceive them to be \"equally\" spaced from each other. Interesting fact, the word \"mel\" actually comes from the word \"melody\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnSkZXaVexhW"
      },
      "source": [
        "display_audio_mel_spectrogram(audio_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0VHyIwVexhY"
      },
      "source": [
        "## Section 3.6 - Generating Our Manifest Files From AN4\n",
        "\n",
        "To train the Nemo model, we must prepare our training and test dataset with manifest files that indicates where to find the audio files, and what the correct transcriptions for each file is. \n",
        "\n",
        "The following cell below lists all the audio files, loads up all the transcripts and then generates the manifest file in the format that is required by the Nemo toolkit.\n",
        "\n",
        "The original AN4 transcriptions look like this:\n",
        "\n",
        "```\n",
        "    <s> YES </s> (an251-fash-b)\n",
        "    <s> GO </s> (an253-fash-b)\n",
        "    <s> YES </s> (an254-fash-b)\n",
        "```\n",
        "\n",
        "We will have to convert it to the manifest required by Nemo in the following format:\n",
        "\n",
        "```\n",
        "    {\"audio_filepath\": \"data/an4/wav/an4_clstk\\\\fash\\\\an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}\n",
        "    {\"audio_filepath\": \"data/an4/wav/an4_clstk\\\\fash\\\\an253-fash-b.wav\", \"duration\": 0.7, \"text\": \"go\"}\n",
        "    {\"audio_filepath\": \"data/an4/wav/an4_clstk\\\\fash\\\\an254-fash-b.wav\", \"duration\": 0.9, \"text\": \"yes\"}\n",
        "```\n",
        "\n",
        "Update the cell below with the folders to original AN4 transcript files, the folders to the audio files, and the output Nemo manifest file. \n",
        "\n",
        "*Training File:*\n",
        "- File is at **'data/train_manifest'**\n",
        "\n",
        "*Test File:*\n",
        "- File is at **'data/test_manifest'**\n",
        "\n",
        "When you are done, run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRM4TJFexhY"
      },
      "source": [
        "# This constructs the manifest for the training data set.\n",
        "#\n",
        "# TODO: Update the code below to indicate where our transcripts, WAV files, and\n",
        "#       and where we should save the manifest file.\n",
        "#\n",
        "build_manifest_for_an4_dataset('data/an4/etc/an4_train.transcription','data/an4/wav/an4_clstk', '???')\n",
        "print(\"Training manifest created.\")\n",
        "\n",
        "# This constructs the manifest for the test data set.\n",
        "#\n",
        "# TODO: Update the code below to indicate where our transcripts, WAV files, and\n",
        "#       and where we should save the manifest file.\n",
        "#\n",
        "build_manifest_for_an4_dataset('data/an4/etc/an4_test.transcription','data/an4/wav/an4test_clstk', '???')\n",
        "print(\"Test manifest created.\")\n",
        "\n",
        "print(\"Completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M5Nj-j9exhb"
      },
      "source": [
        "## Section 3.7 - Training a Model from Scratch\n",
        "\n",
        "Training an Automatic Speech Recognition model takes a lot of time. Fortunately, the AN4 dataset is small enough for us to train our model quickly for demonstration purposes.\n",
        "\n",
        "Update the parameters to indicate where to load the model's configuration file (configured to use a Jasper architecture), the training data and validation data manifest files.\n",
        "\n",
        "The Jasper architecture looks like the following:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/day2-pm/jasper.png\" />\n",
        "\n",
        "https://arxiv.org/pdf/1904.03288.pdf\n",
        "\n",
        "Update the cell below to specify the training and test manifest files.\n",
        "\n",
        "*Training File:*\n",
        "- File is at **'data/train_manifest'**\n",
        "\n",
        "*Test File:*\n",
        "- File is at **'data/test_manifest'**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kCpN6gNexhc"
      },
      "source": [
        "# Update the paths to the training and test manifest files.\n",
        "#\n",
        "create_speech_recognition_nemo_model('configs/config.yaml', '???', '???')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGVAv5Omexhd"
      },
      "source": [
        "Update the batch size below to **8** and the number of epochs to **30**. \n",
        "\n",
        "Smaller batch sizes in training allow your model to converge quickly, but take slightly longer to train per epoch. Large batch sizes run quicker, but your model converges slower.\n",
        "\n",
        "Then, run the cell below to see how the training happens. \n",
        "\n",
        "If you run this with Jupyter Notebook (not Jupyter Lab), you will be able to see the training loss and the training progresses. Pay attention to the reducing loss as the training continues. Typically, for a good Speech Recognition model in Nemo for this AN4 dataset, the loss should be about 10 or below. The lower the loss, the better the prediction confidence.  \n",
        "\n",
        "If you are in Jupyter Lab, you may not be able to see the training loss, so just wait for all the training epochs to complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvvN-hu8exhe",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Adjust the batch size between 4, 8, or 16, and set the number of epochs to 30.\n",
        "# \n",
        "train_speech_recognition_nemo_model(\n",
        "    0,                                   # Batch size\n",
        "    1,                                   # Number of GPUs\n",
        "    0)                                   # Max number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oVip-yEexhg"
      },
      "source": [
        "You can save the trained model to a file, and reload it up later!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZRfzeSNexhg",
        "scrolled": true
      },
      "source": [
        "save_speech_recognition_nemo_model('models/first_asr_model.nemo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsORQRrGexhi"
      },
      "source": [
        "You can then load up the model from that saved file at any time later without re-training again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq0TQRiUexhi"
      },
      "source": [
        "load_speech_recognition_nemo_model('models/first_asr_model.nemo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K_jwhRDexhk"
      },
      "source": [
        "To determine how well our model is working, we compute the Word Error Rate by taking the validation data and getting our new speech recognition model to transcribe all audio files in our validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAoKHjpexhl",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Compute the Word Error Rate based on the testing data.\n",
        "# \n",
        "compute_wer('???')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN7UHNSgexhn"
      },
      "source": [
        "## Section 3.8 - Use the Pre-Trained Model\n",
        "\n",
        "We can improve the model by changing the model parameters or training the model for more epochs.\n",
        "\n",
        "We can also try to use the pre-trained model (the one we used above) to see if the results are any good on this custom AN4 dataset. After all, the pre-trained model provided by Nvidia has been trained on a much large dataset and is able to capture more variations for the same words. \n",
        "\n",
        "Update the following cell to: \n",
        "1. Use the configuration file at **\"configs/config.yaml\"**\n",
        "1. Download the pre-trained model **\"QuartzNet15x5Base-En\"**.\n",
        "2. Call the compute_wer function with the new model and the params.\n",
        "\n",
        "NOTE: Once again, NVidia provides a list of models that you can use. Refer to the link to find out more: https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le95X1h6exhn",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Set the path to the configuration file and the model name\n",
        "# NOTE: The details of the neural network specified in the config file is\n",
        "#       ignored in this case, because we will be downloading the pre-trained neural network.\n",
        "#\n",
        "create_speech_recognition_nemo_model_pretrained('configs/config.yaml', '???')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8R8GpO9exhp"
      },
      "source": [
        "Run the following cell below to see what happens when we use a pre-trained model on our AN4 validation dataset.\n",
        "\n",
        "The Word Error Rate for the pre-trained model may average at about 13%, and is not as good as we expected, given that the model has been pre-trained. It could very well be due to the fact that the AN4 \"spelling\" task is very different from the conversational audio datasets used to train that model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24u_BShBexhq",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Update the following line to compute the WER on our test dataset\n",
        "#\n",
        "compute_wer('???')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePQcKIvPexhr"
      },
      "source": [
        "## Section 3.9 - Using Transfer Learning a Pre-Trained Model\n",
        "\n",
        "Since the downloaded model has been previously trained to transcribe English text using a very large speech corpus, it would have learned to model a lot of features about the English language. \n",
        "\n",
        "We can take advantage of that to improve the Word Error Rate of our custom AN4 recognition task by further training the downloaded QuartzNet model with the custom AN4 dataset.\n",
        "\n",
        "In the following cell, set the learning rate to 0.001. This value is actually lower than the default learning rate of 0.01 we used to train a new model from scratch. The rationale for this is that since the model is already pre-trained, we do not need the model to make big adjustments during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlQhCPKaexhs"
      },
      "source": [
        "# TODO:\n",
        "# Update the learning rate to 0.001\n",
        "#\n",
        "set_learning_rate(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsY7tQvcexht"
      },
      "source": [
        "Update the following cell to set the batch size to **8**, and the number of epochs to **3**, and then run the cell.\n",
        "\n",
        "You will notice that the training takes a little longer per epoch, compared to our previous model, because this model is large and more trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRTAnIUexhu",
        "scrolled": true
      },
      "source": [
        "set_train_test_manifest('data/train_manifest', 'data/test_manifest')\n",
        "\n",
        "# TODO:\n",
        "# Update the batch size and the number of epochs.\n",
        "#\n",
        "train_speech_recognition_nemo_model(\n",
        "    0,                                   # Batch size\n",
        "    1,                                   # Number of GPUs\n",
        "    0)                                   # Max number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enzXNNkqexhw"
      },
      "source": [
        "Once again, update and run the following cell to use our newly-trained QuartzNet model to compute the Word Error Rate.\n",
        "\n",
        "With the custom training that we have done, the Word Error Rate on your test data drops dramatically. You can try to run the cell above to train the model for a few more epochs to reduce the Word Error Rate. You may find that the Word Error Rate can drop down to about 2%!\n",
        "\n",
        "Try reducing the learning rate to an even lower value than 0.001 and train your model again. You may see your Word Error Rate fall to near 0%!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi6pPIZYexhw",
        "scrolled": true
      },
      "source": [
        "# TODO:\n",
        "# Update the following line to compute the WER on our test dataset\n",
        "#\n",
        "compute_wer('???')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGwpCDpexhy"
      },
      "source": [
        "Run the following the save the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq7KlWdoexhz"
      },
      "source": [
        "save_speech_recognition_nemo_model('models/improved_quartznet_model.nemo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alhTsPg2exh1"
      },
      "source": [
        "Once again, you can load up the improved QuartzNet model trained on the AN4 dataset at any time later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALXmVNYaexh1",
        "scrolled": true
      },
      "source": [
        "load_speech_recognition_nemo_model('models/improved_quartznet_model.nemo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S__tSU4bexh3"
      },
      "source": [
        "## Section 3.10 - Testing Your Model\n",
        "\n",
        "This is how we can perform a transcription on a WAV file using our newly trained QuartzNet model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGBfijy5exh3",
        "scrolled": true
      },
      "source": [
        "perform_transcription_on_file('data/an4/wav/an4_clstk/mgah/cen2-mgah-b.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adRxI1SNexh5"
      },
      "source": [
        "Now, try to record some audio of yourself spelling out words or saying numbers.  \n",
        "\n",
        "Run the following cell to display an audio recorder panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODX7z5yBexh5"
      },
      "source": [
        "display_audio_recorder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VLHSA-Hexh7"
      },
      "source": [
        "Then, update the cell below to pass in the **save_recorded_audio()** function into the perform_transcription_on_file() function.\n",
        "\n",
        "And run the cell to transcribe what you have recorded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR1DOV_Lexh8"
      },
      "source": [
        "# TODO:\n",
        "# Pass the function 'save_recorded_audio()' into the parameter but without\n",
        "# the single quotation marks \"'\"\n",
        "#\n",
        "perform_transcription_on_file(save_recorded_audio())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGc0lMygexh-"
      },
      "source": [
        "Try also to record conversational speech.\n",
        "\n",
        "Discuss about what happened to your newest model when trying to transcribe conversational speech compared to the original unaltered pre-trained model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZkxR9kexh_"
      },
      "source": [
        "## Section 3.11 - Explore helpers.py\n",
        "\n",
        "Do take a look at the helpers.py file to see how we processed files and used Nemo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGn04Stjexh_"
      },
      "source": [
        "## Section 3.12 - Challenge\n",
        "\n",
        "Try to use your model to transcribe and perform the sentiment analysis using the Text Classification models that we have created in the earlier labs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhO5HcrexiA"
      },
      "source": [
        "## Section 3.13 - Further Reading\n",
        "\n",
        "The Nvidia Nemo library provides many neural network models for Natural Language Processing, Speaker Recognition, Text-to-Speech. You can find out more about Nemo here: \n",
        "- https://github.com/NVIDIA/NeMo\n",
        "\n",
        "This exercise was also heavily adapted from the tutorial provided by Nemo that you can find here:\n",
        "- https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/asr/01_ASR_with_NeMo.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfVjk1abexiA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}